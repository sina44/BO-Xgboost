# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1smzdnno3VCaQMlAUzOaB0DuefRbC63no
"""

#Bayesian Xgboost
from hyperopt import hp, fmin, tpe, STATUS_OK, Trials
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score,roc_auc_score,matthews_corrcoef, cohen_kappa_score, brier_score_loss, f1_score
space={'max_depth': hp.quniform("max_depth", 1, 10, 1),
        'gamma': hp.uniform ('gamma', 0.01,10),
        'colsample_bytree' : hp.uniform('colsample_bytree', 0.01,1),
        'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1)),
        'n_estimators': hp.uniform('n_estimators',150,300)
        ,'thershold': hp.uniform('thershold', 0.35,0.65)
        ,'subsample': hp.uniform('subsample', 0.01,1)
    }
def objective(space):
    clf=XGBClassifier(n_estimators =int(space['n_estimators']), max_depth = int(space['max_depth'])
    , gamma = space['gamma'],colsample_bytree=space['colsample_bytree'],learning_rate=space['learning_rate'],subsample=space['subsample'])
    evaluation = [( train_X, train_Y), ( test_X, test_Y)]
    
    clf.fit(train_X,train_Y)
    pred = clf.predict(test_X)
    auc=roc_auc_score(test_Y,pred)
    pred=pred>space['thershold']
    accuracy = accuracy_score(test_Y,pred)
    acc=accuracy_score(test_Y,pred)
    mcc=matthews_corrcoef(test_Y,pred)
    ck=cohen_kappa_score(test_Y,pred)
    Br=brier_score_loss(test_Y,pred)
    F1=f1_score(test_Y,pred)
    pred3=pred.flatten()
    test_Y1=test_Y.to_numpy()
    test_Y1=test_Y1.flatten()

    w=EMPChurn(test_Y1,pred3)
    ww=w.empc()
    print(classification_report(test_Y,pred),'Accuracy:',acc,' AUC:',auc,' MCC:',mcc,' Kappa_score:',
      ck,' Brier_score:',Br,' F1_score:', F1, ' EMPC:',ww)
    print ("SCORE:", accuracy)
    #change the metric if you like
    return {'loss': -accuracy, 'status': STATUS_OK ,'Accuracy':acc,'AUC':auc,'Max_Depth':space['max_depth'],'Gamma':space['gamma'],'Colsample_Bytree':space['colsample_bytree'],'Learning_rate':space['learning_rate'],'N_Estimators':space['n_estimators'],'Thershold':space['thershold'],'SubSample':space['subsample']}


trials = Trials()
best = fmin(fn=objective,
            space=space,
            algo=tpe.suggest,
            max_evals=200,
            trials=trials)

print (best)

#bayes_trials_results = sorted(trials.results, key = lambda x: x['loss'])
bayes_trials_results = trials.results
opt=pd.DataFrame(bayes_trials_results)
                                              
opt.head()
opt['loss']= -1*opt['loss']
opt['index']=opt.index
#opt.to_csv('D:bxgboost1.csv')